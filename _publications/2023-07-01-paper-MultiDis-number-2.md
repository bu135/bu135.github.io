---
title: "Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text"
collection: publications
excerpt: 'This paper analyze the complementary characteristic of two methods in language out-of-distribution (OoD) detection: training a language model from scratch and fine-tuning a pre-trained language model using ID examples. It introduces a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Extensive experiments over multiple benchmark datasets show its state-of-the-art performance. We also explore its application as an AIGC detector to distinguish answers generated by ChatGPT and human experts, showing that our model exceeds human evaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.'
date: 2023-07-01
venue: 'Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics'
paperurl: 'https://aclanthology.org/2023.acl-long.403/'
---

This paper analyze the complementary characteristic of two methods in language out-of-distribution (OoD) detection: training a language model from scratch and fine-tuning a pre-trained language model using ID examples. It introduces a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Extensive experiments over multiple benchmark datasets show its state-of-the-art performance. We also explore its application as an AIGC detector to distinguish answers generated by ChatGPT and human experts, showing that our model exceeds human evaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.

[Download paper here](https://aclanthology.org/2023.acl-long.403/)
